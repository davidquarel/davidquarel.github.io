---
title: I blame the tokenizer
date: 2024-10-01
tags:
  - ramble
published: true
---

So I'm working in a replication of a paper that claims the [Llama-2 family of models "think in English"](https://arxiv.org/abs/2402.105880). 
They show that if you use [logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) during a forward 
pass on a multi-shot translation prompt from, say, French to German, internally the model assigns a high probability
to the corresponding English word. That's very curious!

I want to query `meta-llama/llama-2-7b-hf` with a large amount of responses of the form 
```
Fran√ßais: " jour" - Deutsch: " Tag"
Fran√ßais: " homme" - Deutsch: " Mann"
Fran√ßais: " cinq" - Deutsch: " f√ºnf"
Fran√ßais: " nouveau" - Deutsch: " neu"
Fran√ßais: " livre" - Deutsch: "
```
The model receives pairs of identical words in French and German (e.g *jour* and *Tag* mean *day* in French and German respectively), 
so naturally the next thing it should guess is <code> Buch</code>  (book), and it does, because models are good at in-context prediction.


That works, but I have lots of examples, and I want to run them in bulk. I note that all the prompt share the same prefix
```
Fran√ßais: " jour" - Deutsch: " Tag"
Fran√ßais: " homme" - Deutsch: " Mann"
Fran√ßais: " cinq" - Deutsch: " f√ºnf"
Fran√ßais: " nouveau" - Deutsch: " neu"
Fran√ßais: "
```

and all the suffixes are of the form[^space]
```
 livre" Deutsch: "
 nuage" Deutsch: "
 sac" Deutsch: "
 montagne" Deutsch: "
 tissu" Deutsch: "
```

[^space]: The space is important, as the tokenizer for `llama-2` likes to encode words with a leading space character, and I need to account for that.

Future words cannot change the predictions for past words, so we can run the model on the common prefix, [cache the activations for keys and values](https://github.com/davidquarel/llm-latent-language/blob/aa6e24e1756773f8bd1d8f47e106887e443053e3/src/kv_cache.py#L39C5-L39C17), 
and then [do a batched run on all the suffixes together](https://github.com/davidquarel/llm-latent-language/blob/aa6e24e1756773f8bd1d8f47e106887e443053e3/src/kv_cache.py#L59C5-L59C22). 
Problem is that not all suffixes are the same number of tokens, so I can't just take the last output of each, as a matrix can't have some rows shorter than other rows.

Okay, no problem, we can tokenize in bulk, and get it to just pad out the shorter sequences with a dummy token, and the tokenizer will tell me with the mask which ones are actually important.

For example, the string `"the quick brown fox"` gets tokenized as `[1, 278,  4996, 17354,  1701, 29916]` and `"a"` becomes `[1, 263]` which gets padded to `[1, 263, 2, 2, 2, 2]`. The mask tells me which parts
of the token are padding or not.


<figure style="text-align: center;">
  <img src="{% link assets/images/llm/tok_pad.png %}" alt="QuAC Hardware Schematic">
  <figcaption style="margin-top: 10px;">Figure 1: Batch tokenization with padding. </figcaption>
</figure>


GPU is kept busy as I can now run inference on many sequences in a big batch rather than for-looping on each row, and I can sum the rows in the attention mask to pick out the prediction I want.

```python
from collections import namedtuple

TokenizedSuffixesResult = namedtuple('TokenizedSuffixesResult', 
                                     ['input_ids', 'attention_mask', 'indices'], 
                                     defaults=[None, None, None])

def tokenize_suffixes(suffixes : List[str], model):
    device = next(model.parameters()).device
    model.tokenizer.pad_token = model.tokenizer.eos_token
    suffix_tokens, attn_mask = model.tokenizer(suffixes,
                                                add_special_tokens=False,
                                                return_tensors="pt",
                                                padding=True).values()
    indices = attn_mask.sum(dim=1)-1
    assert torch.all(indices >= 0), "Attention mask has zeros, empty suffixes"
    suffix_tokens = suffix_tokens.to(device)
    
    return TokenizedSuffixesResult(
        input_ids=suffix_tokens,
        attention_mask=attn_mask,
        indices=indices
    )
```

`llama-2` likes to put a special reserved start-of-sequence token `<s>` with token id `1` at the 
start of each sequence. Good for a prompt, bad for a suffix of a prompt. No worries, use `add_special_tokens=False` for the tokenizer.

**BUT** for some inexplicable reason, it also likes to prepend a space character to every sequence BEFORE tokenizing[^tok].
Okay, sure, just slice out the first column. But due to tokenizer shenanigans, depending on what the first word is, 
the space might bind to it and tokenize as  `‚ñÅhello`, or sometimes seperately as `‚ñÅ` `world`. The model internally 
learns to do this based on the data it's seen: some words have spaces in front, some don't, and it's learned to tokenize
based on that.

[^tok]: [Try it out for yourself!](https://tiktokenizer.vercel.app/?model=codellama%2FCodeLlama-7b-hf) See the difference between tokenizing `hello`, which gets preprocessed as `‚ñÅhello` and tokenized as `22172`, whereas üåç gets preprocessed as `‚ñÅüåç` and tokenized as `29871,31494` i.e. the space token `‚ñÅ` with id `29871`, followed by the earth token `üåç` with id `31494`.

So, shit, I can't just slice it out. So, I need to find something I can add to the front that will never bind to a space. So I search through all 50k tokens in the 
vocabulary, and by happenstance üåç[^earth] is a single token with token id `31494`, as opposed to a sequence of utf-8 encoded bytes, which is typical for emojis.

<details>
<summary>Example for üòÇ</summary>
For example, üòÇ gets tokenized as <code>243, 162, 155, 133</code>, which (almost!) matches the utf-8 encoding.
<pre><code class="language-python">>>> bytes = "üòÇ".encode("utf-8")
>>> [int(x) for x in bytes]
[240, 159, 152, 130]
</code></pre>
Notice how everything is off by three? It's because they added some special tokens at the start, <code>&lt;unk&gt;</code> for unknown,
<code>&lt;s&gt;</code> for beginning and <code>&lt;/s&gt;</code> for end of text tokens, and shuffled everything else down.
We can see this by running
<pre><code class="language-python">from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

vocab = tokenizer.get_vocab()
sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])

for token, id in sorted_vocab[:10]:
print(f"Token: {token}, ID: {id}")
</code></pre>
which gives
<pre><code>Token: &lt;unk&gt;, ID: 0
Token: &lt;s&gt;, ID: 1
Token: &lt;/s&gt;, ID: 2
Token: &lt;0x00&gt;, ID: 3
Token: &lt;0x01&gt;, ID: 4
Token: &lt;0x02&gt;, ID: 5
Token: &lt;0x03&gt;, ID: 6
Token: &lt;0x04&gt;, ID: 7
Token: &lt;0x05&gt;, ID: 8
Token: &lt;0x06&gt;, ID: 9
</code></pre>
</details>

[^earth]: yes, the literal emoji üåç

Ergo, I present to you [my hacky solution](https://github.com/davidquarel/llm-latent-language/blob/aa6e24e1756773f8bd1d8f47e106887e443053e3/src/llm.py#L159), for which the emoji üåç is critical, and it **WILL NOT WORK** with a 
different choice of emoji [^earth2]

[^earth2]: No, not even other rotations of the earth emoji, like üåé or üåè. It has to be üåç.

```python
def safe_tokenize(suffixes : List[str] | str, 
                  model : HookedTransformer
) -> TokenizedSuffixesResult:
    device = next(model.parameters()).device
    model.tokenizer.pad_token = model.tokenizer.eos_token
    
    if isinstance(suffixes, str):    
        suffixes = [suffixes]
    
    if "Llama-2" in model.tokenizer.name_or_path:
        suffixes = ["üåç" + x for x in suffixes]
        space_token_id = model.tokenizer.convert_tokens_to_ids("‚ñÅ")
        earth_token_id = model.tokenizer.convert_tokens_to_ids("üåç")
        
        suffix_tokens, attn_mask = model.tokenizer(suffixes,
                                                add_special_tokens=False,
                                                return_tensors="pt",
                                                padding=True).values()
        
        assert torch.all(suffix_tokens[:, 0] == space_token_id), "llama2 has leading space token"
        assert torch.all(suffix_tokens[:, 1] == earth_token_id), "llama2 single token for üåç"
        
        suffix_tokens = suffix_tokens[:, 2:]
        attn_mask = attn_mask[:, 2:]
        idx = attn_mask.sum(dim=-1) - 1 #-1, and another two more: one for the space token, one for the üåç token
    
    else: # models that do not add leading spaces
        suffix_tokens, attn_mask = model.tokenizer(suffixes,
                                                add_special_tokens=False,
                                                return_tensors="pt",
                                                padding=True).values()
        idx = attn_mask.sum(dim=-1) - 1
        
    assert torch.all(idx >= 0), "Attention mask has zeros, empty suffixes"
    suffix_tokens = suffix_tokens.to(device)
    attn_mask = attn_mask.to(device)
    idx = idx.to(device)
    
    return TokenizedSuffixesResult(
        input_ids=suffix_tokens,
        attention_mask=attn_mask,
        indices=idx
    )
```

You prepend `üåç`, knowing now that the tokenizer will slap on a space character `‚ñÅ` and then tokenize
the result. `‚ñÅüåç` is not in the vocabulary, but both `‚ñÅ` and `üåç` are, and there are no other strings in the vocabulary
that contain `üåç` as a substring, so the only possible valid tokenization is `29871, 31494,...` followed by the result of the tokens.
Then, just slice out first two columns of spaces and üåç's!

God help me.